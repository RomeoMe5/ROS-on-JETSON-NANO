# SaS_homework
## Общая информация
В данной работе была реализована установка Robotic Operating System на плату Nvidia Jetson Nano, а также запущен проект, демонстрирующий вычислительные мощности данной платы, а также ее возможности по управлению периферией. В качестве такого проекта была выбрана известная игра “Камень, ножницы, бумага”. При этом плата распознает жест, показанный игроком посредством информации полученной с веб-камеры, демонстрирует собственный случайный жест на распечатанном на 3D принтере манипуляторе, имитирующем руку человека.
## Процесс установки
#### Установка и первичная настройка ОС на  Jetson Nano
Процесс установки операционной системы на данную плату схож с установкой системы на Raspberry Pi. Сама операционная система представляет собой адаптированную под процессор платы Ubuntu 18.04. Для запуска ОС на плате необходима MicroSD карта на 32 гб.

Для установки ОС необходимо выполнить следующие шаги:
- Скачать SD Card Formatter (https://www.sdcard.org/downloads/formatter/) для форматирования карты.
- Скачать balenaEtcher (https://www.balena.io/etcher/) для записи образа ОС на карту.
- Скачать образ операционной системы с официального сайта Nvidia (https://developer.nvidia.com/jetson-nano-sd-card-image).
- Форматировать карту.
- Записать образ на карту при помощи BalenaEtcher.
- Подключить к плате питание 5 В/ 3 А.
- Пройти процедуру первичной настройки. Задать имя пользователя, пароль, необходимые раскладки.

Jetson Nano в стандартной комплектации не имеет WiFi модуля, поэтому для подключения к сети существует три пути:
- Купить модуль Intel (https://aax-us-east.amazon-adsystem.com/x/c/QimjWna0Q8TuPEPRIumVsmEAAAFw5AiB8QEAAAFKAbq_T7M/https://assoc-redirect.amazon.com/g/r/https://amzn.to/2YTckpz?linkCode=w61&imprToken=g61usD0rEtA9Qb69-sU4EQ&slotNum=2)  и установить его по данному туториалу (https://www.jetsonhacks.com/2019/04/08/jetson-nano-intel-wifi-and-bluetooth/).
- Купить USB WiFi адаптер, совместимый с Raspberry Pi, и использовать его.
- Подключаться при помощи кабеля Ethernet.
#### Установка ROS Melodic на Jetson Nano
ROS (Robotic Operating System) - это гибкая платформа (фреймворк) для разработки программного обеспечения роботов. Это набор разнообразных инструментов, библиотек и определенных правил, целью которых является упрощение задач разработки ПО роботов.

Создание действительно надежного, универсального программного обеспечения для роботов чрезвычайно сложная задача. С точки зрения робота, проблемы, которые кажутся тривиальными для людей, часто требуют очень сложных технических решений. Часто разработка такого решения не под силу одному человеку.

ROS была создана, чтобы стимулировать совместную разработку программного обеспечения робототехники. Каждая отдельная команда может работать над одной конкретной задачей, но использование единой платформы, позволяет всему сообществу получить и использовать результат работы этой команды для своих проектов.

Для установки ROS Melodic на Jetson Nano нужно выполнить следующие последовательные шаги:
- Настроить устройство для приема ПО с packages.ros.org: для этого необходимо выполнить следующую команду в терминале устройства:
```
$ sudo sh -c 'echo "deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main" > /etc/apt/sources.list.d/ros-latest.list'
```
- Добавить ключ командой:
```
$ sudo apt-key adv --keyserver 'hkp://keyserver.ubuntu.com:80' --recv-key C1CF6E31E6BADE8868B172B4F42ED6FBAB17C654
```
- Обновить индексы apt:
```
$ sudo apt update
```
- Установить ROS Melodic в нужной комплектации (комплектация  full содержит средства для моделирования поведения робота, и очень требовательна к ресурсам, поэтому рекомендуется устанавливать версию desktop).
```
$ sudo apt install ros-melodic-desktop
```
- Для инициации ROS необходимо ввести следующие команды.
```
$ sudo rosdep init 
$ rosdep update
```
- Настраиваем автоматический запуск ROS в каждой новой сессии shell.
```
$ echo "source /opt/ros/melodic/setup.bash" >> ~/.bashrc 
$ source ~/.bashrc
```

Далее необходимо настроить рабочее пространство. Это каталог, в котором необходимо создавать и настраивать пользовательские пакеты. Компилятор ROS корректно работает лишь с данным каталогом. Таких папок может быть несколько, но единовременно можно работать лишь с одной. Настройка производится следующими шагами:
- Установим необходимые библиотеки для корректной работы ROS.
```
$ sudo apt-get install cmake python-catkin-pkg python-empy python-nose python-setuptools libgtest-dev python-rosinstall python-rosinstall-generator python-wstool build-essential git
```
- Создадим корневой каталог рабочего пространства и папку для хранения пакетов. Затем перейдем в корневой каталог.
```
$ mkdir -p ~/catkin_ws/src 
$ cd ~/catkin_ws/
```
- Настроим рабочую директорию командой 
```
$ catkin_make
```
- Добавим путь к файлу настроек в bashrc.
```
$ echo "source ~/catkin_ws/devel/setup.bash" >> ~/.bashrc 
$ source ~/.bashrc
```

Установка завершена. Для проверки правильности установки можно запустить одну из команд ROS(roscd (переход в рабочую папку), roscore (запуск ядра ROS)).
Также, для корректной работы необходимо установить tensorflow2 (https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform/index.html).

## Основные понятия ROS.
Главными актантами в структуре пакета ROS являются Node и Topic. Под Node подразумевается полноценный кусок кода на одном из языков программирования (Python, C++). Данный кусок кода может выполнять свою конкретную задачу, иметь связь с другими Node. Логично, что куски кода должны уметь обмениваться между собой информацией, сообщениями. Для этого существует другая структура ROS: Topic. Topic это поле определенного типа, которое создается в Node, которая в свою очередь публикует туда сообщения. Например, в случае данного проекта Node cv_camera публикует изображение, полученное с веб камеры в topic, на который подписан модуль распознавания жестов и алгоритма самой игры. Как только информация в топике обновляется, запускается функция в подписанном на него модуле. На один топик может быть подписано несколько нодов. При этом, каждый нод является самостоятельным процессом, поэтому работа их идет параллельно, начинаясь по заданному условию, и по изменению сообщения в топике, на который подписан нод. Такой подход позволяет выстраивать независимые архитектуры роботизированных систем, например, система обработки препятствий и алгоритма встроенной игры в роботе.

#### Система распознавания жестов при помощи камеры.
Для того, чтобы система смогла корректно распознать жест, выкинутый пользователем использовалась предобученная нейронная сеть от Google, созданная как демонстрация принципов работы фреймворка для создания вычислительных потоков Mediapipe. Данная модель доступна для загрузки и представляет собой файл в формате Tflite. Система, используемая в проекте для распознавания жестов использует сразу три предобученных модели. Первая модель обнаруживает на изображении кисть руки и выдает в качестве предсказания прямоугольник, внутри которого по ее мнению находится кисть пользователя на изображении. Данный кусок изображения передается в следующую модель, детектирующую на изображении 21 ключевую точку на руке пользователя, и возвращающая в качестве предсказания вектор их координат на картинке.

Третья модель является простой моделью логистической регрессии, выдающей в качестве ответа вероятность принадлежности объекта к тому или иному классу. Очевидно, что при демонстрации различных жестов их ключевые точки достаточно хорошо линейно разделимы, поэтому данная модель показывает неплохое качество работы. В файле rock_paper_scissors_2d_landmark.ipynb проводится обучение модели регрессии на три жеста: камень, ножницы и бумага.
Следует поподробнее описать метод распознавания жестов с камеры. Для реализации данной цели проект использует две модели. Результатом работы первой модели являются вектора, сформированные из координат. 

Для реализации данной модели был разработан программный модуль, содержащий в себе функции для обработки фреймов, поступающих с камеры. Наиболее важной является функция get_dataset(), с помощью которой происходит формирование выходных векторов. Полностью алгоритм работы данного программного модуля состоит в следующем:
- Выделение и сохранение ключевых точек (на рисунке выше выделены цветом) для каждого из поступающих фреймов.
- Вычисление выходных векторов посредством вычитания из координаты каждой ключевой точки координаты опорной точки (нижняя точка ладони).
- Формирование данных для обучения – добавление целевого признака (метки класса) к выходным векторам.

Данный программный модуль полезен, так как позволяет пользователем самим собирать данные для обучения итоговой второй модели, а также менять жесты в зависимости от поставленной перед ними задачи.

Следующим этапом данные, сформированные программным модулем, описанным выше, поступают на вход модели машинного обучения (в данном случае, логистическая регрессия), которая на основе входных данных определяет показываемый жест. Результаты работы всего проекта приведены ниже:

Необходимо отметить, что сбор данных для распознавания жестов (ножницы, бумага, камень) проводился студентами бригады. Для добавления собственных жестов необходимо выполнить часть Collecting dataset в файле rock_paper_scissors_2d_landmark.ipynb для собственных жестов, а затем сохранить полученную модель.  На данную модель необходимо будет заменить файл logregest_1200.pickle в папке nodes/py2-RPS репозитория. Далее необходимо исправить логику определения победителя в файле ./nodes/py2-RPS/RockPaperScissorsClass.py в функциях num_to_gesture() и winner_result(), выработав собственные правила превосходства жестов друг над другом.

#### Проект RPS.
Проект с игрой расположен в репозитории (https://github.com/pavllan90/ROS_RPS_Mediapipe). Он представляет собой готовый пакет под ROS. Опишем ноды, входящие в данный пакет.
- py2-RPS
Модуль, подгружающий модели, выполняющий основные вычисления, а также реализующий в себе алгоритм самой игры.
- ServoController
Модуль, выводящий жест, выкинутый компьютером на распечатанный на 3D  принтере манипулятор, подключенный по i2c шине через шилд PCA9685. Инструкции по подключению расположены в статье (https://www.jetsonhacks.com/2019/07/22/jetson-nano-using-i2c/).
- ImageOut
Модуль, дублирующий жест на экран, выводя картинку с жестом компьютера, определенный компьютером жест пользователя, а также выводящий результат самой игры.

Для того чтобы запустить данный пакет необходимо склонировать репозиторий в папку src вашей рабочей дериктории, затем запустить команду catkin_make в дериктории catkin_ws. Далее необходимо запустить следующую команду:
```
$ roslaunch ROS_RPS_Mediapipe RPS.launch
```
Данный способ запустит все необходимые ноды, перечисленные в файле RPS.launch.

#### Сравнение Jetson Nano и Raspberry Pi 3.
Для объективного сравнения взяты тесты из статьи (https://habr.com/ru/post/460723/). В данной статье автор проводит тест на программе, рассчитывающей синусы.
```
# SimpleSpeedTest.py
# https://wiki.tiker.net/PyCuda/Examples/SimpleSpeedTest

import pycuda.driver as drv
import pycuda.autoinit
from pycuda.compiler import SourceModule
import numpy
import time


blocks = 64
block_size = 128
nbr_values = blocks * block_size
n_iter = 100000

print("Calculating %d iterations" % (n_iter))
print()

#####################
# SourceModule SECTION

# create two timers so we can speed-test each approach
start = drv.Event()
end = drv.Event()

mod = SourceModule("""__global__ void gpusin(float *dest, float *a, int n_iter)
                      {
                          const int i = blockDim.x*blockIdx.x + threadIdx.x;
                          for(int n = 0; n < n_iter; n++) {
                            a[i] = sin(a[i]);
                          }
                          dest[i] = a[i];
                      }""")

gpusin = mod.get_function("gpusin")

# create an array of 1s
a = numpy.ones(nbr_values).astype(numpy.float32)
# create a destination array that will receive the result
dest = numpy.zeros_like(a)

start.record() # start timing
gpusin(drv.Out(dest), drv.In(a), numpy.int32(n_iter), grid=(blocks,1), block=(block_size,1,1) )
end.record() # end timing
# calculate the run length
end.synchronize()
secs = start.time_till(end)*1e-3
print("PyCUDA time and first three results:")
print("%fs, %s" % (secs, str(dest[:3])))
print()

#############
# CPU SECTION
# use numpy the calculate the result on the CPU for reference

a = numpy.ones(nbr_values).astype(numpy.float32)
t1 = time.time()

for i in range(n_iter):
    a = numpy.sin(a)

print("CPU time and first three results:")
print("%fs, %s" % (time.time() - t1, str(a[:3])))

```
Результат сравнения достаточно впечатляющ для одноплатного компьютера. 
Jetson nano — 0.67c GPU, 13.3c CPU.
Raspberry Pi 3B+ — 41.85c CPU, GPU — данных нет, CUDA на RPi не работает.
Ноутбук — 0.05с GPU, 3.08c CPU.
Конечно, результат Jetson Nano не дотягивает до мощностей полноценного компьютера, но демонстрирует многократный прирост скорости в сравнении с Raspberry Pi 3.
По субъективным ощущениям, устройство также сильно превосходит Raspberry Pi, по скольку на второй частота кадров составляла примерно один кадр на 3-4 секунды, а на первой около 5-10 кадров в секунду.


## Итоги.
Данная работа является исследованием возможностей одноплатного компьютера с использованием Deep Learning и Robotic Operating System. В конечном итоге была реализована игра Камень, ножницы, бумага с использованием компьютерного зрения. Помимо этого была подключена переферия, ввиде пяти сервоприводов, подключенных через шину i2c. Данное задание показало, что мощности Jetson Nano вполне достаточно для достаточно сложных и вычислительно трудоемких задач машинного обучения и глубокого обучения. Также было приведено сравнение Jetson Nano и Raspberry Pi 3. Данное сравнение показывает превосходство платы от Nvidia над Raspberry. Также была проведена установка ROS на Jetson Nano и протестированы возможности данного фреймоврка. В итоге был разработан пакет под ROS, осуществляющий алгоритмы игры.
